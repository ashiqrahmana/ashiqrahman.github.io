<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Ashiq Rahman Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
		<header id="header">
			<nav>
				<ul>
					<li><a href="index.html">Home</a></li>
					<li><a href="#RP">Research Projects</a></li>
					<li><a href="#GP">Graduate Projects</a></li>
					<li><a href="#UP">Undergraduate Projects</a></li>
					<li><a href="#PP">Personal Projects</a></li>
				</ul>
			</nav>
		</header>

		<!-- Sidebar -->
		<section id="sidebar">
			<div class="inner">
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>						
						<li><a href="about.html">About Myself</a></li>
						<li><a href="resume.html">Resume</a></li>
						<li><a href="internships.html">Work Experience</a></li>
						<li><a href="patents.html">Patents</a></li>
						<li><a href="skills.html">Skills</a></li>
					</ul>
				</nav>
			</div>
		</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="RP" class="wrapper">
						<div class="inner">
							
							<h2 class="major">Research Projects</h2>
							<h3>Video VPR</h3>
							<p>The Video Visual Place Recognition project aims to create a robust and efficient system that can recognize and match specific places in videos. By extracting distinctive visual features, representing them, and employing matching algorithms, the system can identify locations across different videos. It also focuses on localization, mapping, and handling challenges like changes in lighting and occlusions. The project's applications include autonomous navigation, augmented reality, and video surveillance. The goals include developing an accurate system, optimizing processing speed, evaluating performance, and exploring incremental learning techniques. Overall, this project aims to advance computer vision technology and its practical applications in diverse domains.</p>
							
							<p><span class="image right"><img src="images\project\gripper_hardware.jpg" alt="" height="300"  /></span></p> 
							<h3>Flexible gripper</h3>
							<p><span class="image right"><img src="images\project\gripper picture.jpeg" alt="" /></span>This project focuses on the development of a bio-inspired two-fingered gripper with a Fin Ray (fish fin) structure and grooves on the contact side. The gripper is composed of soft cross beams that were optimized using the finite element method. Three different geometries were considered: Even, Uneven, and Semi-filled configurations. The grippers were 3D printed using Polyurethane material. The gripper with uneven cross beams showed controlled displacement and had the ability to handle various geometries and materials within its gripper length and payload capacity. The project aims to enhance the compliance and dexterity of robotic manipulators, making them safer for interaction with humans. The gripper design was inspired by fish fins and the Fin Ray Effect was utilized to improve grasping ability. The experiment involved simulation analysis using Inventor 2021 and physical testing using an articulated robot. The results demonstrated the effectiveness of the gripper in gripping different objects.</p>
						</div>
					</section>

					<section id="GP" class="wrapper">
						<div class="inner">
							
							<h2 class="major">Graduate Projects</h2>							
							
							<h3>Brick Laying Robot</h3>
							<p>
								<span class="image right"><img src="images\project\brick_laying.jpeg" alt="" height="250" /></span>
								Designed and implemented a robotic system integrating UR5 manipulator and AgileX Tracer for automated construction tasks.
							</p>
							  <ul>
								<li>Developed algorithms for seamless and synchronized control of both robots.</li>
								<li>Enabled precise bricklaying capabilities.</li>
								<li>Utilized a 24-volt DC power system for efficient operation.</li>
							  </ul>
							<p>Skills: Robotics integration, motion control, algorithm development, automation</p>
							  

							<p><span class="image right"><img src="images\project\Husky-XArm Work.jpeg" alt="" height="300"  /></span></p> 
							<h3>DoorDash Delivery Robot</h3>
							<p>
								<span class="image right"><img src="images\project\Husky-XArm.png" alt=""  height="350"/></span>
								Developed a versatile robotic delivery system by: Integrating a Husky mobile robot with a Xarm manipulator arm.
							</p>
							<ul>
							<li>Successfully combined the mobility platform of Husky with the manipulation capabilities of Xarm for a comprehensive solution.</li>
							<li>Enabled functionalities for:</li>
								<ul>
									<li>Automated door opening</li>
									<li>Package delivery</li>
									<li>Object manipulation across different rooms</li>
								</ul>
							</ul>
							<p>Skills: Robot integration, path planning, object manipulation, automation</p>

							<h3>Mobile Object Rearrangement</h3>
							<p>
							<span class="image right"><img src="images\project\MOR_1cube.jpg" alt=""  height="350"/></span>
							Developed a camera-guided robot for autonomous 2D structure building through mobile manipulation. Tackled adaptability, collision avoidance, and precise object placement. Implemented navigation & manipulation algorithms, explored reinforcement learning for adaptation, built a simulation environment, and achieved successful real-world testing. This experience highlights:
							</p>
							<ul>
								<li>My ability to design and develop robotic systems</li>
								<li>Expertise in robot navigation and manipulation algorithms</li>
								<li>Proficiency in simulation and real-world testing</li>
							</ul>
							<p>Skills: Pose Net Neural Network, Object manipulation, Real World data collection</p>

							<h3>Build Manipulation</h3>
							<p>
							<span class="image right"><img src="images\project\BM_run.gif" alt=""  height="350"/></span>
							In this project, I Led the development of a camera-guided robot for autonomous 2D structure building. Tackled challenges like adaptability, collision avoidance, and precision. Built a simulation environment, implemented navigation/manipulation algorithms, and achieved successful real-world testing. Highlights include:
							</p>
							<ul>
								<li>Reinforcement learning for autonomous navigation and structure construction</li>
								<li>Algorithmic solutions for adaptability & collision avoidance</li>
								<li>Environment modifications to enhance object pushing efficiency</li>
								<li>Validation through simulation & real-world testing</li>
							</ul>
							<p>Skills: Reinforcement Learning(PPO Clip), A* Path Planning, Object manipulation, PoseNet Neural Network</p>
							<ul class="actions">
								<li><a href="https://drive.google.com/drive/folders/1jyKwQPIe25W-5M9p9kyjhZG8nbJiWw-b?usp=drive_link" class="button primary">Learn More</a></li>
							</ul>

							<h3>Maze Mapping and Navigation</h3>
							<p>
							<span class="image right"><img src="images\project\maze_robot.gif" alt=""  height="350"/></span>
							In this project, maze mapping and navigation is acheived developing a sophisticated algorithm tailored to tackle the challenges of maze-like indoor environments. some of the algorithms that were used halped to map the maze and effectively navigate through the setting autonomously with little to no human inteference.
							</p>
							<ul>
								<li>Utilization of advanced computer vision techniques like SIFT (Scale-Invariant Feature Transform) and SuperGlue, accurately identify target locations amidst complex maze layouts and parallely used for mapping the maze setting.</li>
								<li>Leveraed the A* path planning algorithm to autonomlusly plan paths from start locations to identifeid target location (global planner)</li>
								<li>Vanishing points mehod was used to effectevielt present he robot from colliding with the walls (local planner)</li>
							</ul>
							<p>Skills: A* Path Planning, Localisation and Navigation, SIFT/SURF, Vanishing points, Camera collibration</p>
							<ul class="actions">
								<li><a href="https://github.com/satyapalsinh10/Maze_Mapping_and_Navigation" class="button primary">Learn More</a></li>
							</ul>
							
							<h3>Tracking using YOLO-v7</h3>
							<p><span class="image right"><img src="images\project\tracking-gif.gif" alt="" /></span>This project uses YOLOv7, a fast and accurate real-time object detection algorithm, to track cars on a highway.

The repository includes images (potentially used for training/testing) and videos demonstrating the tracking results.
You can explore the code to understand how YOLOv7 is implemented for this specific task.
YOLOv7 excels in real-time applications due to its single forward pass processing and achieves high accuracy and speed in object detection. This project showcases its versatility for object tracking.</p>
							<p>Skills: Machine Learning</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Tracking-using-YOLO-v7/tree/main" class="button primary">Learn More</a></li>
							</ul>
							
							<h3>Miniature AGV for material handling</h3>
							<p><span class="image right"><img src="images\project\MIni AGV run.gif" alt="" /></span>The Automated Guided Vehicle (AGV) is a revolutionary solution for material handling and transportation. It was inspired by the industrial forklift and designed to move pallets from one location to another using just a few inputs from the user. The AGV offers many benefits over traditional manual labor, including increased efficiency, reduced risk of accidents and damage, and reduced costs.</p>
							<p>Some of the key features of our scaled automated AGV are</p>
							<ul>
								<li>Minimal user input</li>
								<li>Intuitive GUI</li>
								<li>Navigation using the onboard encoder and IMU sensors</li>
								<li>Efficient and safe movement of pallets</li>
								<li>Compact size and mobility</li>
							</ul>
							<p>Skills: Teamwork · Kalman filtering · Simulations</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Miniature-AGV-for-material-handling" class="button primary">Learn More</a></li>
							</ul>
							
							<h3>WidgetMover AGV</h3>
							<p><span class="image right"><img src="images\project\picknddrop.jpeg" alt="" /></span>WidgetMover is an autonomous guided vehicle designed for efficient factory operations. It navigates a layout with three lanes, discovers widgets in lane A, and moves them to specific locations in lane B. Utilizing the A* algorithm, WidgetMover dynamically generates optimal paths, adapting to obstacles encountered using ultrasonic sensors. Equipped with QTI infrared sensors for line following and Parallax Continuous Servo actuators, the robot efficiently maneuvers through intersections while obeying lane constraints. By streamlining widget transportation and adaptive navigation, WidgetMover optimizes factory operations and enhances productivity.</p>
							<p>Some of the key features of our WidgetMover are</p>
							<ul>
								<li>Provide Pick and Place position</li>
								<li>Uses Button to collect input and displays the status appropriately</li>
								<li>Navigation using the IR sensors and ultrasonic sensors</li>
								<li>Uses A* for navigating from one point to another making it adaptable to any situation</li>
							</ul>
							<p>Skills: Teamwork · A* · Propellor Board</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/WidgetMover-Autonomous-Guided-Vehicle-for-Efficient-Factory-Operations" class="button primary">Learn More</a></li>
							</ul>

							<h3>AutoInventoryBot</h3>
							<p><span class="image right"><img src="images\project\monitoring bot.jpeg" alt="" /></span>The AutoInventoryBot is an autonomous robot designed for warehouse inventory monitoring and management. The project aims to improve the efficiency and accuracy of inventory control by using robotics technology. The robot is equipped with hardware components such as QTI sensors for line-following, servo motors for motion control, a Parallax Propeller microprocessor for managing actions, a Raspberry Pi for processing camera data, a Pi camera for visual servoing, and a power distribution board for connecting the components. The robot uses ArUco tags to identify and count defective and non-defective widgets in the warehouse. It navigates the warehouse floor based on directional markers and counts the widgets at each station. The robot's goal is to reach its final destination and display the inventory count. The project aims to demonstrate how robotics can streamline inventory management processes and reduce the need for manual labor in warehouses.</p>
							<p>Some of the key features of our scaled automated AGV are</p>
							<ul>
								<li>Minimal user input</li>
								<li>Visual Servoing</li>
								<li>Navigation using QTI Sensor and A*</li>
								<li>Compact size and mobility</li>
							</ul>
							<p>Skills: Teamwork · Kalman filtering · Simulations</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/AutoInventoryBot-Autonomous-Robot-for-Warehouse-Inventory-Monitoring-and-Management" class="button primary">Learn More</a></li>
							</ul>

							<h3>AutoParkBot</h3>
							<p><span class="image right"><img src="images\project\AutoParker.jpeg" alt="" /></span>The project aims to develop an autonomous robot that can identify vacant parking spaces in a parking lot and navigate along a predetermined path. The robot utilizes sensors to detect lines on the ground and ultrasonic sensors to determine the presence of a vehicle in a parking space. The information is stored in the Arduino Uno microcontroller's memory and displayed using LEDs. The project utilizes QTI sensors for line following, ultrasonic sensors for obstacle detection, servo motors for controlling the robot's movement, and an Arduino Uno microcontroller as the brain of the system. The report discusses the motivation behind the project, its objectives, the hardware components used, and the potential future scope of the project.</p>
							<p>Some of the key features of our WidgetMover are</p>
							<ul>
								<li>The process is completely Autonomous with little-no human intervention</li>
								<li>Uses Button to collect input and displays the status appropriately</li>
								<li>Appropriate binary displays for indicating the numbers of cars and bikes</li>
							</ul>
							<p>Skills: Teamwork · Line Following · Arduino Board · Binary Display</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/AutoParkBot-Intelligent-Autonomous-Parking-System" class="button primary">Learn More</a></li>
							</ul>
							<!--h3>Aruco-Cube</h3>
							<p><span class="image right"><img src="images\project\aruco%20cube%201.jpg" alt="" /></span>This project is centered around tag-based augmented reality, leveraging the capabilities of the pyAprilTag package. The key objective is the detection of ArUco markers within an image, utilizing camera calibration parameters to overlay a 3D cube onto these markers. The result is a dynamic and real-time augmented reality visualization from various perspectives. The primary goal is to demonstrate the enchantment of augmented reality through ArUco markers, enriching visual experiences and laying the groundwork for continued exploration in this exciting field.</p>
							<p>Skills: Aruco tags · AR · Camera callibration</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Aruco-Cube" class="button primary">Learn More</a></li>
							</ul>
							
							<h3>Checkerboard-Callibration</h3>
							<p><span class="image right"><img src="images\project\checkerboard_image.png" alt="" /></span>The project centers on camera calibration using a checkerboard pattern, employing NumPy and OpenCV for the calibration process. It delivers crucial parameters like the camera matrix (K matrix) and the top two distortion parameters (k1 and k2). The calibration workflow encompasses capturing video from the camera, detecting checkerboard corners, and computing camera calibration results. Through this implementation, the project plays a key role in bolstering the fundamental elements of computer vision, ensuring meticulous and accurate camera calibration for diverse applications.</p>
							<p>Skills: OpenCV · Camera callibration</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Checkerboard-callibration" class="button primary">Learn More</a></li>
							</ul>
							
							<h3>Low Dimensional Projection using Autoencoder</h3>
							<p><span class="image right"><img src="images\project\output.jpg" alt="" /></span>This project focuses on low-dimensional projection using autoencoders and t-SNE, implemented in Python. The code utilizes TensorFlow and scikit-learn to train an unsupervised learning neural network (autoencoder) on the Fashion-MNIST dataset. The autoencoder generates a lower-dimensional representation of the images, and t-SNE is then applied to further reduce the dimensionality to 2. The results are visualized in a scatter plot, where each point represents an image's lower-dimensional representation, with colors based on ground-truth labels. This project demonstrates the capability of autoencoders and t-SNE for effective dimensionality reduction and visualization of complex datasets.</p>
							<p>Skills: Machine Learning · Autoencoders · TensorFlow</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Autoencoders" class="button primary">Learn More</a></li>
							</ul>
							<h3>Sherlock Holmes Decode</h3>
							<p><span class="image right"><img src="images\project\result_decoded_image.jpg" alt="" /></span>This project revolves around the decoding of a hidden message left by Detective Sherlock for his assistant, Dr. Watson. The code, implemented in Python using OpenCV and NumPy, allows users to manipulate the Hue, Saturation, and Value (HSV) values of the image to reveal the secret message. The interactive thresholding feature uses trackbars, enabling users to manually select HSV ranges and isolate specific parts of the image based on color. The decoded image with the revealed message can be found in the results folder of the repository. This project showcases an intriguing and interactive approach to uncovering hidden information within an image, inspired by the world of Sherlock Holmes.</p>
							<p>Skills: OpenCV · HSV</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Sherlock-Detection---HSV" class="button primary">Learn More</a></li>
							</ul-->
						</div>
					</section>

					<section id="UP" class="wrapper">
						<div class="inner">
							<h2 class="major">Undergraduate Projects</h2>
							
							<h3>Autonomous Package Drone Delivery</h3>
							<p><span class="image right"><img src="images\project\Autonomous Drone.jpeg" alt="" /></span>The use of autonomous drones for package delivery within a college campus is an innovative and cutting-edge solution that offers many advantages over traditional methods. The autonomous drone is designed to deliver packages from one point to another within the college campus, all while avoiding obstacles and ensuring safe and efficient delivery.</p>

							<p>Some of the notable features of the bot are:</p>
							<ul>
								<li>Obstacle avoidance capability using stereo-cameras for depth perception and a ToF sensor for altitude determination.</li>
								<li>GPS for robot localization and path planning using the A* algorithm.</li>
								<li>Efficient and reliable delivery with no need for manual control or intervention.</li>
								<li>Compact size and mobility for flexible and versatile deployment.</li>
								<li>Improves efficiency and productivity by reducing the risk of accidents and damage.</li>
								<li>Reliable and cost-effective solution for package delivery within the college campus.</li>
								<li>Custom app for notifying the user of the status.</li>
								<li>Battery level indicator to calculate the flight path accordingly</li>
							</ul>
							<p>Skills: Teamwork · Control Systems · Kalman filtering · Sensor Fusion · SLAM · Path Planning · Simulation</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Autonomous-Package-Delivery-Drone" class="button primary">Learn More</a></li>
							</ul>
							
							
							<h3>3 UPS 1 UPU Parallel Manipulator</h3>
							<p><span class="image right"><img src="images\project\Parallel Manipulator.jpeg" alt="" /></span>Parallel manipulators are becoming increasingly popular in various industries due to their unique design, high precision, and ability to manipulate objects in tight spaces. The 3-UPS 1-UPU parallel manipulator, also known as a 5-link manipulator, is a type of parallel manipulator that offers several advantages over other types of manipulators.</p> 
							<p><span class="image right"><img src="images\project\pm_run.gif" alt="" /></span></p> 
							<p>The features and benefits of the 3-UPS 1-UPU parallel manipulator are as follows:</p>
							<ul>
								<li>High precision and accuracy in object manipulation.</li>
								<li>Ability to manipulate objects in tight spaces.</li>
								<li>Real-time feedback allows for accurate and precise control.</li>
								<li>Electric prismatic actuation provides precise control over the movement.</li>
								<li>Flexible movement allows for easy navigation in tight spaces.</li>
								<li>Ideal solution for industrial and commercial applications requiring precision and maneuverability in tight spaces.</li>
								<li>Composed of 5 links connected by active and passive joints for flexible movement.</li>
								<li>Real-time feedback provided by sensors on the position and orientation of the manipulator's end-effector.</li>
							</ul>
							<p>Unique design allows for use in tight spaces like borewells, manholes, and septic tanks</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/3UPS-1UPU-Parallel-Manipulator" class="button primary">Learn More</a></li>
							</ul>

							<h3>E-Transmission</h3>
							<p><span class="image right"><img src="images\project\E trans.jpeg" alt="" /></span>E-transmission is a cutting-edge technology that revolutionizes the way vehicles are powered and transmitted. It provides numerous benefits to drivers and vehicle owners alike, making the driving experience smoother, more efficient, and more reliable.</p>
							<p>Some of the key features of E-transmission include:</p>
							<ul>
							  <li>Advanced non-contact electromagnetic transmission</li>
							  <li>Independent engine speed control, allowing for smooth power delivery even under varying road conditions and speeds</li>
							  <li>Occupies less space than conventional transmission systems</li>
							  <li>Increases the life of the vehicle by eliminating gears and clutches</li>
							  <li>Improves the efficiency and reliability of power transmission from the engine to the road</li>
							  <li>Reduces fatigue loading on the engine, resulting in longer engine life</li>
							  <li>Combines the efficiency and range of an electric vehicle with the reliability of a traditional engine car</li>
							</ul>
							<p>E-transmission provides a new and innovative solution for modern vehicles, offering a range of benefits over traditional transmission systems. Whether it be improved driving experience, increased efficiency, or longer vehicle life, the benefits of E-transmission are clear, and it is likely to play an important role in the future of the automotive industry.</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/E-Transmission" class="button primary">Learn More</a></li>
							</ul>
							
							<p><span class="image right"><img src="images/project/ilmbt_ros_obstacle avoidance.gif" alt="" height="300"  /></span></p> 
							<h3 class="major">Intelligent Line Marking Bot</h3>
							<p><span class="image right"><img src="images\project\line marking bot.jpeg" alt="" height="300" /></span>The Intelligent Line Marking Bot, a type of Automated Guidance Vehicle (AGV), is a robot designed to efficiently mark field lines on any sports field. The AGV is equipped with a range of onboard sensors, including GPS, IMU, and encoder, which work together to provide reliable data that is used for path planning and localization.</p>
							<p>Some of the notable features are:</p>
							<ul>
							  <li>AGV is capable of marking field lines on any sports field</li>
							  <li>Uses GPS, IMU, and encoder sensors for input from the robot for state estimation and motion planning</li>
							  <li>Sensors are fused to provide reliable data for path planning and localization of the robot</li>
							  <li>The system was implemented in Matlab-Simulink and tested first and then the prototype was built and run on the field using ROS melodic in Raspberry Pi.</li>
							  <li>The bot was capable of 4-wheel steering and drive making it capable of navigating over any terrain with ease and not comprising accuracy.</li>
							</ul>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Intelligent-Line-Marking-Bot" class="button primary">Learn More</a></li>
							</ul>
							
							<h3 class="major">Machinehole Scavenger</h3>
							<p><span class="image right"><img src="images\project\Machine hole bot.jpeg" alt="" /></span>MachineHole-Scavenger is an innovative mechatronic system designed to efficiently clean manhole sewage waste using advanced sensors and actuators. The project aims to eliminate the risks associated with manual scavenging by implementing a robotic solution. With a combination of encoders, moisture sensors, and gas sensors, the system can monitor the environment and the robot itself, providing a high degree of flexibility. Users can control the complex mechatronic system easily through a simple button and joystick configuration, thanks to kinematics and dynamic modeling. The mechanical aspects involve synchronized mechanisms, such as an independent jack for X and Y motion, a leadscrew-powered prismatic joint for z-axis actuation, and a separate actuator for rotation. The MachineHole-Scavenger's design ensures efficient and effective cleaning of manhole sewage waste.</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/MachineHole-Scavenger" class="button primary">Learn More</a></li>
							</ul>

							<h3 class="major">Intelligent Picking - Flipkart Grid 2.0</h3>
							<p><span class="image right"><img src="images\project\Intelligent Picking.jpeg" alt="" /></span>The project is based on automating the process of picking and placing objects in the manufacturing and warehouse industry. It involves the use of a camera, an articulated arm, and optimized algorithms to pick heavy payloads accurately and efficiently. The system employs path-planning techniques to navigate the arm to the desired location while taking into account obstacles in its way.</p>
							<ul>
							  <li>Intelligent picking system using an onboard camera</li>
							  <li>Articulated arm with optimized design to pick heavy payloads</li>
							  <li>Path planning to achieve accurate and efficient movement</li>
							  <li>Kinematics involved in the arm movement</li>
							  <li>PID control for precise control and stability while picking and placing objects</li>
							</ul>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Intelligent-Picking" class="button primary">Learn More</a></li>
							</ul>
							
							<p><span class="image right"><img src="images\project\Borewell Rescue Bot Work.jpeg" alt="" height="300"  /></span></p> 
							<h3 class="major">Borewell Rescue Bot</h3>
							<p><span class="image right"><img src="images\project\Borewell Rescue Bot.jpeg" alt="" /></span>An innovative solution was developed to address the pressing issue of animals and infants getting trapped in deep holes. The rescue bot was designed and coded with a control system to ensure the successful retrieval of these victims. The bot was designed with various features that allow it to navigate through difficult terrains, reach the victims, and safely lift them to the surface. The control system of the bot was developed by integrating various sensors, actuators, and algorithms that allowed it to make real-time decisions and execute precise movements.</p>
							<ul>
							  <li>Wireless RF connection between the controller and the bot</li>
							  <li>Use of 5 DOF Parallel Manipulator used to maximize the reach of the limited workspace</li>
							  <li>Real-time feedback on the environment using various sensors</li>
							  <li>Real-time feedback on the bot pose</li>
							</ul>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Borewell-Rescue-Bot" class="button primary">Learn More</a></li>
							</ul>
						</div>
					</section>

					<section id="PP" class="wrapper">
						<div class="inner">							
							<h2 class="major">Personal Projects</h2>

							<h3 class="major">Animetronic Hand</h3>
							<p><span class="image right"><img src="images\project\Assembled_hand.png" alt="" /></span>The Animetronic-Hand project aimed to develop a gesture-controlled animetronic hand for teleoperation, focusing on enhancing accessibility for differently-abled individuals. It was created as a proof of concept during the "Byte into Hardware" Hardware Hackathon at NJIT. The project utilized computer vision techniques for gesture tracking and interpretation, allowing users to control the hand's movements. Hardware components included a 3D printed animetronic hand, Tower Pro Servos, a Logitech webcam, Arduino, and an IMU. The hand's 3D model was split and printed on multiple printers due to its long print duration. Gesture tracking and control relied on the Mediapipe package to locate and calibrate keypoints, enabling accurate actuation commands for the animetronic hand. The project's architecture successfully integrated computer vision, hardware components, and calibration techniques to create an interactive and accessible system.</p>
							<ul class="actions">
								<li><a href="https://github.com/govind-aadithya/Animetronic-Hand" class="button primary">Learn More</a></li>
							</ul>

							<h3 class="major">Smart Bin</h3>
							<p>The Smart-Bin project focuses on developing an intelligent dustbin capable of sorting waste into four categories: Domestic, Metal, Plastic, and Glass. The system utilizes an ESP32 microcontroller and a camera module to capture images of the trash deposited by the user. These images are then sent to a central computer, which runs a machine learning model designed to classify the waste into the appropriate category. The output from the model determines the bin in which the waste should be placed.</p>
							<p>To ensure reliable classification, the Smart-Bin project incorporates proper lighting conditions to optimize image quality and enhance the accuracy of the waste sorting process. By combining the capabilities of the ESP32 microcontroller, camera module, and machine learning model, the Smart-Bin aims to automate waste sorting and promote efficient recycling practices.</p>
							<ul class="actions">
								<li><a href="https://github.com/ashiqrahmana/Smart-Bin" class="button primary">Learn More</a></li>
							</ul>

							<h3 class="major">Single Point Self Balanced guided vehicle powered by Gyroscope with high maneuverability.</h3>
							<p>It is an innovative object that is designed to provide a high degree of maneuverability, stability, and safety. It is equipped with sensors and control systems that help it maintain its balance and stability, even in the face of changing conditions. The self-balanced guided vehicle powered by a drone gives a high degree of accurate control on the plane assisted with a rotating disc giving gyroscopic stability.</p>
						</div>
					</section>

			</div>

		<!-- Footer -->
		<footer id="footer" class="wrapper style1-alt">
			<div class="inner">
				<ul class="menu">
					<li> PORTFOLIO. DESIGN BY: ASHIQ RAHMAN ANWAR BATCHA</li>
				</ul>
			</div>
		</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
